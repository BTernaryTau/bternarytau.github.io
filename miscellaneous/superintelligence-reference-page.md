---
layout: default
title: "Superintelligence reference page"
description: "A collection of links to notable pages on superintelligence and AI alignment."
---
# {{ site.title }}
## {{ page.title }}
### Introductions

Superintelligence is a very broad and complex topic. These resources are for those who are mostly or entirely unfamiliar with the subject.

* The [Superintelligence FAQ](https://slatestarcodex.com/superintelligence-faq/) by Scott Alexander of Slate Star Codex is an introduction to the dangers of superintelligence in the form of answers to "frequently asked questions".
* Tim Urban of Wait But Why writes about superintelligence in two parts.
	* [The AI Revolution: The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) deals with how we might get to superintelligence from the AI systems we have now.
	* [The AI Revolution: Our Immortality or Extinction](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html) discusses the possible consequences of superintelligence.
	* There's also [A reply to Wait But Why on machine superintelligence](https://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/) by Luke Muehlhauser, which clarifies and corrects various details of the two posts.
* [Smarter Than Us](https://smarterthan.us/) is a book by Stuart Armstrong explaining why AI is likely to eventually become smarter than humans and the problems that this poses.
* [Extinction Risk from Artificial Intelligence](https://aisafety.wordpress.com/) is a website created by Michael Cohen as an introduction to superintelligence and the risks it poses to the human species.

### Alignment problem

The alignment problem is the problem of aligning an AI's goals with the values of humanity. Solving this problem is often believed to be critical in ensuring that superintelligence has a positive impact on the world.

* [AI Alignment: Why It’s Hard, and Where to Start](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/) is a talk by Eliezer Yudkowsky available in both video and transcript form. It covers topics like subproblems in AI alignment, why alignment is both difficult and necessary, and what progress has already been made.
* Yudkowsky also has [a shorter post](https://www.facebook.com/yudkowsky/posts/10154083549589228) that covers the necessity of alignment, the difficulty of alignment, and why the alignment problem is not self-solving.
* [Of Myths And Moonshine](https://www.edge.org/conversation/the-myth-of-ai#26015) by Stuart Russell articulates the case for emphasizing the alignment problem in AI research.

### Intelligence explosion

An intelligence explosion is a hypothetical scenario in which a self-improving intelligence is able to improve itself more and more rapidly thanks to the very improvements it makes. The feasibility of this scenario has great implications for how superintelligence may arise.

* MIRI's [Intelligence Explosion FAQ](https://intelligence.org/files/IE-FAQ.pdf) answers questions regarding the intelligence explosion concept, superintelligence, and other related ideas.
* [Intelligence Explosion: Evidence and Import](https://intelligence.org/files/IE-EI.pdf) by Luke Muehlhauser and Anna Salamon argues that it is feasible for human-level AI to be developed before 2100 and for an intelligence explosion to follow shortly after.

### Slow takeoff

A slow takeoff, sometimes called a soft takeoff, is a scenario in which a self-improving intelligence improves itself slowly enough for humans to intervene during the process. This is the opposite of the fast or hard takeoff experienced in the intelligence explosion scenario.

* Victoria Krakovna's [Risks from general artificial intelligence without an intelligence explosion](https://vkrakovna.wordpress.com/2015/11/29/ai-risk-without-an-intelligence-explosion/) lists reasons why an AI improvement scenario lacking an intelligence explosion still comes with many risks.
* [Decisive Strategic Advantage without a Hard Takeoff](https://kajsotala.fi/2016/04/decisive-strategic-advantage-without-a-hard-takeoff/) by Kaj Sotala explores the possibility of an AI acquiring control of the world absent an intelligence explosion.
* [Existential risk from AI without an intelligence explosion](http://alexmennen.com/index.php/2017/05/25/existential-risk-from-ai-without-an-intelligence-explosion/) by Alex Mennen covers reasons an AI might gain a decisive strategic advantage prior to undergoing an intelligence explosion and how such scenarios differ from the reverse situation.

### Orthogonality thesis

The orthogonality thesis states that, barring a few edge cases, any level of intelligence can be combined with any terminal goal. This precludes scenarios where, for example, sufficiently smart superintelligences will automatically replace any arbitrary goals they were initially given with a goal of behaving morally.

* Bostrom lays out the orthogonality thesis in his paper [The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents](https://nickbostrom.com/superintelligentwill.pdf).
* Arbital has an [Orthogonality Thesis](https://arbital.com/p/orthogonality/) page that covers arguments for and implications of the orthogonality thesis.
* [The Orthogonality Thesis, Intelligence, and Stupidity](https://www.youtube.com/watch?v=hEUO6pjwFOo) is a video by Robert Miles that explains the orthogonality thesis using Hume's guillotine.

### Instrumental convergence thesis

The instrumental convergence thesis states that there are instrumental goals that will be useful to agents with a wide variety of terminal goals. Self-preservation, resource acquisition, and self-improvement are all examples of convergent instrumental goals.

* [The Basic AI Drives](https://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/) by Stephen Omohundro lays out the reasoning behind why a wide range of terminal goals would lead to similar instrumental goals, establishing the groundwork for the instrumental convergence thesis.
* Bostrom's [The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents](https://nickbostrom.com/superintelligentwill.pdf) formulates the instrumental convergence thesis alongside the orthogonality thesis.
* Arbital's [Instrumental convergence](https://arbital.com/p/instrumental_convergence/) page gives a fairly technical introduction to the claims made by the instrumental convergence thesis.
* Miles explains why convergent instrumental goals arise and how this allows powerful agents' behaviors to be predicted in the video [Why Would AI Want to do Bad Things? Instrumental Convergence](https://www.youtube.com/watch?v=ZeecOKBus3Q).

### Timelines

When will superintelligence be created? The answer to this question is important because it determines how long there is to prepare for its arrival. As such, it is worth looking to predictions of when various AI milestones will be reached.

* AI Impacts has a [Guide to pages on AI timeline predictions](https://aiimpacts.org/guide-to-pages-on-ai-timeline-predictions/) which is primarily focused on timelines for pre-superintelligence milestones but also includes some superintelligence timelines.
* [Future Progress in Artificial Intelligence: A Survey of Expert Opinion](https://nickbostrom.com/papers/survey.pdf) by Vincent C. Müller and Nick Bostrom includes questions about timelines for "high–level machine intelligence" and superintelligence.
