---
layout: default
title: "Superintelligence reference page"
---
# {{ site.title }}
## {{ page.title }}
### Introductions

Superintelligence is a very broad and complex topic. These resources are for those who are mostly or entirely unfamiliar with the subject.

* The [Superintelligence FAQ](https://slatestarcodex.com/superintelligence-faq/) by Scott Alexander of Slate Star Codex is an introduction to the dangers of superintelligence in the form of answers to "frequently asked questions".
* Tim Urban of Wait But Why writes about superintelligence in two parts.
	* [The AI Revolution: The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) deals with how we might get to superintelligence from the AI systems we have now.
	* [The AI Revolution: Our Immortality or Extinction](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html) discusses the possible consequences of superintelligence.
	* There's also [A reply to Wait But Why on machine superintelligence](https://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/) by Luke Muehlhauser, which clarifies and corrects various details of the two posts.
* [Smarter Than Us](https://smarterthan.us/) is a book by Stuart Armstrong explaining why AI is likely to eventually become smarter than humans and the problems that this poses.
* [Extinction Risk from Artificial Intelligence](https://aisafety.wordpress.com/) is a website created by Michael Cohen as an introduction to superintelligence and the risks it poses to the human species.

### Alignment problem

The alignment problem is the problem of aligning an AI's goals with the values of humanity. Solving this problem is often believed to be critical in ensuring that superintelligence has a positive impact on the world.

* [AI Alignment: Why It’s Hard, and Where to Start](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/) is a talk by Eliezer Yudkowsky available in both video and transcript form. It covers topics like subproblems in AI alignment, why alignment is both difficult and necessary, and what progress has already been made.
* Yudkowsky also has [a shorter post](https://www.facebook.com/yudkowsky/posts/10154083549589228) that covers the necessity of alignment, the difficulty of alignment, and why the alignment problem is not self-solving.
* [Of Myths And Moonshine](https://www.edge.org/conversation/the-myth-of-ai#26015) by Stuart Russell articulates the case for emphasizing the alignment problem in AI research.

### Timelines

When will superintelligence be created? The answer to this question is important because it determines how long there is to prepare for its arrival. As such, it is worth looking to predictions of when various AI milestones will be reached.

* AI Impacts has a [Guide to pages on AI timeline predictions](https://aiimpacts.org/guide-to-pages-on-ai-timeline-predictions/) which is primarily focused on timelines for pre-superintelligence milestones but also includes some superintelligence timelines.
* [Future Progress in Artificial Intelligence: A Survey of Expert Opinion](https://nickbostrom.com/papers/survey.pdf) by Vincent C. Müller and Nick Bostrom includes questions about timelines for "high–level machine intelligence" and superintelligence.